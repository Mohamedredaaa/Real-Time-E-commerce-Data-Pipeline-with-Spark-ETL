{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real-Time E-commerce Data Pipeline with Spark ETL\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "In this project, you will design and implement an ETL (Extract, Transform, Load) pipeline for a hypothetical e-commerce platform named **ShopEase**. The platform generates massive amounts of data daily, including user interactions, transactions, and inventory updates. Your task is to process this data using Apache Spark to derive meaningful insights and support real-time analytics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scenario\n",
    "\n",
    "**ShopEase** aims to enhance its data analytics capabilities to improve user experience, optimize inventory management, and increase sales. The data generated includes:\n",
    "\n",
    "- **User Activity Logs:** Clickstream data capturing user interactions on the website.\n",
    "- **Transaction Records:** Details of purchases, refunds, and returns.\n",
    "- **Inventory Updates:** Information about stock levels, restocks, and product information changes.\n",
    "- **Customer Feedback:** Reviews and ratings provided by customers.\n",
    "\n",
    "The company requires a robust ETL pipeline to process this data, perform transformations, and make it available for analytics and reporting in both batch and real-time modes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Requirements\n",
    "\n",
    "You are required to perform the following tasks using Apache Spark (preferably with PySpark or Scala):\n",
    "\n",
    "### 1. Data Ingestion\n",
    "- **Batch Data:**\n",
    "  - Load historical data from large CSV and JSON files stored in your local file system.\n",
    "- **Real-Time Data:**\n",
    "  - Simulate and ingest streaming data from a Kafka topic representing live user activity logs.\n",
    "\n",
    "### 2. Data Processing and Transformation\n",
    "- **Using RDDs:**\n",
    "  - Perform a transformation to filter out any corrupted or incomplete records from the transaction data.\n",
    "  - Implement a custom transformation to anonymize user IDs for privacy compliance.\n",
    "- **Using DataFrames:**\n",
    "  - Clean and standardize inventory data (e.g., handling missing values, normalizing text).\n",
    "  - Join user activity logs with transaction records to analyze user behavior leading to purchases.\n",
    "- **Using Spark SQL:**\n",
    "  - Create temporary views and execute SQL queries to compute:\n",
    "    - Top 10 most purchased products in the last month.\n",
    "    - Monthly revenue trends.\n",
    "    - Inventory turnover rates.\n",
    "\n",
    "### 3. Real-Time Streaming Processing (Optional but Recommended)\n",
    "- Set up a Spark Streaming job to process incoming user activity logs.\n",
    "- Compute real-time metrics such as:\n",
    "  - Active users per minute.\n",
    "  - Real-time conversion rates.\n",
    "  - Detect and alert on unusual spikes in specific user activities.\n",
    "\n",
    "### 4. Data Storage\n",
    "- Store the transformed data into appropriate storage systems:\n",
    "  - Use Parquet format for batch-processed data in a local data lake.\n",
    "  - Use an in-memory data store like Redis or a database like PostgreSQL for real-time metrics.\n",
    "\n",
    "### 5. Performance Optimization\n",
    "- Optimize Spark jobs for better performance by:\n",
    "  - Caching intermediate DataFrames where necessary.\n",
    "  - Tuning Spark configurations (e.g., partition sizes, executor memory).\n",
    "  - Using appropriate join strategies.\n",
    "\n",
    "### 6. Documentation and Reporting\n",
    "- Document the ETL pipeline architecture.\n",
    "- Provide sample dashboards or reports (using Spark's built-in visualization) showcasing the insights derived.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions & Requirements\n",
    "\n",
    "### 1. Data Ingestion\n",
    "- **Q1:** How did you ingest the batch and real-time data? Provide code snippets demonstrating the loading of data using RDDs and DataFrames.\n",
    "\n",
    "### 2. Data Cleaning and Transformation\n",
    "- **Q2:** Describe the transformations applied to the transaction data using RDDs. How did you ensure data quality and privacy?\n",
    "\n",
    "### 3. DataFrame Operations\n",
    "- **Q3:** How did you clean and standardize the inventory data using DataFrames? Provide examples of handling missing values and normalizing text fields.\n",
    "\n",
    "### 4. Spark SQL Queries\n",
    "- **Q4:** Present the Spark SQL queries used to calculate the top 10 most purchased products, monthly revenue trends, and inventory turnover rates.\n",
    "\n",
    "### 5. Real-Time Processing\n",
    "- **Q5:** If implemented, explain how the real-time streaming was set up. What metrics were computed in real-time, and how were they stored/displayed?\n",
    "\n",
    "### 6. Performance Optimization\n",
    "- **Q6:** What strategies did you employ to optimize the performance of your Spark jobs? Provide examples of configuration settings or code optimizations.\n",
    "\n",
    "### 7. Reporting\n",
    "- **Q7:** Show sample outputs or dashboards that visualize the insights derived from the ETL pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ShopEase_ETL\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load transactions data (CSV format)\n",
    "transactions_df = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .load(\"/content/large_transactions.csv\")\n",
    "\n",
    "# Load inventory data (JSON format)\n",
    "inventory_df = spark.read.format(\"json\").option(\"multiline\", \"true\").load(\"/content/large_inventory.json\")\n",
    "\n",
    "# Load customer feedback data (JSON format)\n",
    "feedback_df = spark.read.format(\"csv\").option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") .load(\"/content/large_customer_feedback.csv\")\n",
    "\n",
    "# Displaying a sample of each dataset\n",
    "transactions_df.show(5)\n",
    "feedback_df.show(5)\n",
    "inventory_df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Data Cleaning and Transformation with RDDs\n",
    "# Convert transactions DataFrame to RDD\n",
    "transactions_rdd=transactions_df.rdd\n",
    "cleaned_rdd = transactions_rdd.filter(lambda row: row.transaction_id is not None \n",
    "                                      and row.user_id is not None \n",
    "                                      and row.amount > 0)\n",
    "cleaned_rdd.take(5)\n",
    "# Write Function to Anonymize user IDs using Hashing\n",
    "def anonymize(record):\n",
    "    \n",
    "import hashlib\n",
    "# Function to anonymize user IDs\n",
    "def anonymize_user_id(row):\n",
    "    anonymized_user_id = hashlib.sha256(str(row.user_id).encode()).hexdigest()\n",
    "    return (row.transaction_id, anonymized_user_id, row.product_id, row.quantity, row.amount, row.transaction_date)\n",
    "\n",
    "anonymized_rdd = cleaned_rdd.map(anonymize_user_id)\n",
    "\n",
    "# Convert back to DataFrame\n",
    "anonymized_transactions_df = anonymized_rdd.toDF([\"transaction_id\", \"user_id\", \"product_id\", \"quantity\", \"amount\", \"transaction_date\"])\n",
    "\n",
    "# Show anonymized data\n",
    "anonymized_transactions_df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: DataFrame Operations for Cleaning and Transformation\n",
    "from pyspark.sql.functions import col, lower, trim\n",
    "\n",
    "# Clean inventory data by handling missing values and normalizing text\n",
    "cleaned_inventory_df = inventory_df.dropna(subset=[\"stock_level\"]) \\\n",
    "                                  .withColumn(\"product_name\", lower(trim(col(\"product_name\"))))\n",
    "\n",
    "# Display cleaned inventory data\n",
    "cleaned_inventory_df.show(5)\n",
    "\n",
    "# Perform a join operation to combine data\n",
    "joined_df = anonymized_transactions_df.join(cleaned_inventory_df, on=\"product_id\", how=\"inner\")\n",
    "\n",
    "# Display joined DataFrame\n",
    "joined_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Spark SQL Queries\n",
    "# Create temporary views for SQL queries\n",
    "anonymized_transactions_df.createOrReplaceTempView(\"transactions\")\n",
    "cleaned_inventory_df.createOrReplaceTempView(\"inventory\")\n",
    "joined_df.createOrReplaceTempView(\"joined_data\")\n",
    "\n",
    "\n",
    "# Query: Monthly revenue trends\n",
    "monthly_revenue_df = spark.sql(\"\"\"\n",
    "    SELECT MONTH(transaction_date) AS month, YEAR(transaction_date) AS year, \n",
    "           SUM(amount * price) AS monthly_revenue\n",
    "    FROM joined_data\n",
    "    GROUP BY month, year\n",
    "    ORDER BY year, month\n",
    "\"\"\")\n",
    "monthly_revenue_df.show()\n",
    "\n",
    "# Query: Top 10 most purchased products in the last month\n",
    "top_products_df = spark.sql(\"\"\"\n",
    "    SELECT product_name, SUM(amount) AS total_amount\n",
    "    FROM joined_data\n",
    "    WHERE transaction_date >= '2023-08-01' AND transaction_date <= '2023-08-31'\n",
    "    GROUP BY product_name\n",
    "    ORDER BY total_amount DESC\n",
    "    LIMIT 10\n",
    "\"\"\")\n",
    "top_products_df.show()\n",
    "\n",
    "# Query: Inventory turnover rates (Total Sales per Product)\n",
    "turnover_rate_df = spark.sql(\"\"\"\n",
    "    SELECT product_name, SUM(amount * price) AS turnover_rate\n",
    "    FROM joined_data\n",
    "    GROUP BY product_name\n",
    "    ORDER BY turnover_rate DESC\n",
    "    LIMIT 20\n",
    "\"\"\")\n",
    "turnover_rate_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Real-Time Processing (Optional)\n",
    "from pyspark.sql.functions import window, countDistinct\n",
    "\n",
    "# For demonstration, create a streaming DataFrame from a sample batch dataset\n",
    "streaming_transactions_df = spark.readStream.schema(transactions_df.schema) \\\n",
    "                                           .csv(\"streaming_transactions_folder\")  # Point to a folder with incoming data\n",
    "\n",
    "# Compute real-time metrics (e.g., active users per minute)\n",
    "active_users = \n",
    "\n",
    "# Display active users in real-time (Note: This will print continuously if run with actual streaming data)\n",
    "query = \n",
    "\n",
    "query.awaitTermination()  # Keep the stream running (can be stopped manually)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Performance Optimization Techniques\n",
    "\n",
    "# Caching DataFrames to optimize performance for multiple transformations\n",
    "anonymized_transactions_df.cache()\n",
    "cleaned_inventory_df.cache()\n",
    "joined_df.cache()\n",
    "\n",
    "# Repartition DataFrames to optimize join performance\n",
    "# Choose an optimal number of partitions based on your data size\n",
    "transactions_df_repartitioned = anonymized_transactions_df.repartition(10, \"product_id\")\n",
    "inventory_df_repartitioned = cleaned_inventory_df.repartition(10, \"product_id\")\n",
    "\n",
    "# Use Broadcast Join for small DataFrames (if applicable)\n",
    "# Assuming inventory_df is relatively small compared to transactions_df\n",
    "from pyspark.sql.functions import broadcast\n",
    "joined_df_optimized = transactions_df_repartitioned.join(broadcast(inventory_df_repartitioned), \"product_id\")\n",
    "\n",
    "# Display the optimized joined DataFrame\n",
    "joined_df_optimized.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Store the Transformed Data\n",
    "\n",
    "# Store the cleaned and transformed data in Parquet format\n",
    "anonymized_transactions_df.write.mode(\"overwrite\").parquet(\"file:///home/student/mini_project/anonymized_transactions.parquet\")\n",
    "cleaned_inventory_df.write.mode(\"overwrite\").parquet(\"file:///home/student/mini_project/cleaned_inventory.parquet\")\n",
    "joined_df_optimized.write.mode(\"overwrite\").parquet(\"file:///home/student/mini_project/joined_data.parquet\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Dashboard Outputs\n",
    "- **Top Products Bar Chart:** Displaying the top 10 products with the highest sales.\n",
    "- **Revenue Trend Line Chart:** Showing monthly revenue over the past year.\n",
    "- **Inventory Turnover Heatmap:** Visualizing turnover rates across different product categories.\n",
    "\n",
    "*(Include actual screenshots or detailed descriptions as appropriate.)*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Top Products Bar Chart - Displaying the top 10 products with the highest sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming top_products_df is your DataFrame\n",
    "top_products = top_products_df.toPandas()\n",
    "\n",
    "plt.figure(figsize=(12, 8))  # Adjusted size for better readability\n",
    "plt.bar(top_products['product_name'], top_products['total_amount'], color='skyblue')\n",
    "plt.title(\"Top 10 Products by Sales\")\n",
    "plt.xlabel(\"Product Name\")\n",
    "plt.ylabel(\"Total Sales Amount\")\n",
    "plt.xticks(rotation=90)  # Rotate x-axis labels to be horizontal for better readability\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)  # Added grid for y-axis for better readability\n",
    "plt.tight_layout()  # Adjust layout to fit labels and titles\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Revenue Trend Line Chart - Showing monthly revenue over the past yea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming monthly_revenue_df is your DataFrame\n",
    "monthly_revenue = monthly_revenue_df.toPandas()\n",
    "\n",
    "plt.figure(figsize=(12, 8))  # Adjust size for better readability\n",
    "plt.plot(monthly_revenue['month'], monthly_revenue['monthly_revenue'], marker='o', color='green', linestyle='-', linewidth=2)\n",
    "plt.title(\"Monthly Revenue Trend in 2023\")\n",
    "plt.xlabel(\"Month\")\n",
    "plt.ylabel(\"Revenue\")\n",
    "\n",
    "# Ensure that 'month' column is in a format suitable for plotting\n",
    "# For example, if 'month' is a string, it should be sorted and formatted appropriately\n",
    "plt.xticks(rotation=45)  # Rotate x-axis labels for better readability\n",
    "plt.grid(True, linestyle='--', alpha=0.7)  # Grid for better readability\n",
    "plt.tight_layout()  # Adjust layout to fit labels and titles\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Inventory Turnover Heatmap - Visualizing turnover rates across different product categories\n",
    "Assuming you have a product_category column in the inventory data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming joined_df_optimized is your DataFrame with product_category\n",
    "inventory_turnover = inventory_df_repartitioned.select(\"product_name\", \"product_category\", \"amount\", \"price\") \\\n",
    "    .groupBy(\"product_category\", \"product_name\") \\\n",
    "    .agg({\"amount\": \"sum\", \"price\": \"mean\"}) \\\n",
    "    .withColumn(\"turnover\", col(\"sum(amount)\") * col(\"avg(price)\")) \\\n",
    "    .toPandas()\n",
    "\n",
    "# Pivot the data for the heatmap\n",
    "heatmap_data = inventory_turnover.pivot(index=\"product_category\", columns=\"product_name\", values=\"turnover\")\n",
    "\n",
    "# Create the heatmap\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(heatmap_data, cmap=\"viridis\", annot=True, fmt=\".2f\")\n",
    "plt.title(\"Inventory Turnover Heatmap\")\n",
    "plt.xlabel(\"Product Name\")\n",
    "plt.ylabel(\"Product Category\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission Guidelines\n",
    "- Ensure that your code is well-documented and follows best practices.\n",
    "- Include instructions on how to set up and run your ETL pipeline.\n",
    "- Provide all necessary configurations and dependencies required for execution.\n",
    "- If using external services (like Kafka or Redis), include setup instructions or mock implementations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Resources\n",
    "- [Apache Spark Documentation](https://spark.apache.org/documentation.html)\n",
    "- [Spark Structured Streaming Guide](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html)\n",
    "- [Optimizing Spark Performance](https://spark.apache.org/docs/latest/tuning.html)\n",
    "- [Using Spark SQL](https://spark.apache.org/docs/latest/sql-programming-guide.html)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
